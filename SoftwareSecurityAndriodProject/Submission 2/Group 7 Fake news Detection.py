# -*- coding: utf-8 -*-
"""SMM Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18P2YpJUkBZ4NTSvuntGAWzoXVmEfZKcW

[link text](https://)
0 -  False
1 - Misleading
2 - True
3 - Unproven
"""

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
import nltk
data= pd.read_csv('/content/train.csv')
data

#Bar plot representation of the given dataset
import seaborn as sb
def create_distribution(dataFile):
    
    return sb.countplot(x='Label', data=dataFile, palette='hls')
    
create_distribution(data)

#Pie chart visualization
print(data["Label"].value_counts())

data.groupby('Label').size().plot(kind='pie',
                                       y = "Label",
                                       label = "Labels",
                                       autopct='%1.1f%%')

data.skew().sort_values()

#Printing the info of the columns
print(data.isna().sum())
print(data.info())
print(data.columns)

# Cleaning the data - Removing missing values and duplicates
def inds_nans(df):
    inds = df.isna().any(axis=1)
    return inds

def inds_dups(df):
    inds = df.duplicated()
    return inds

train_data_clean = data.loc[~((inds_nans(data))|(inds_dups(data))),:]
train_data_clean
# train_data_clean has the same number of rows as our original data hence there are no missing values or duplicates

#Visualizing the sources with maximum truths
import matplotlib.pyplot as plt
data['Source'].value_counts()
ListSource = data['Source'].unique()
dict1={}
for i in ListSource:
  k = data[(data['Source']==i) & (data['Label']==2)].shape[0]
  dict1[i] = k 
print(dict1)  

from matplotlib.pyplot import figure

figure(figsize=(100,20), dpi=75)

x_axis=[]
y_axis =[]
for i in dict1.keys():
  x_axis.append(i)
  y_axis.append(dict1[i])
y_axis.sort(reverse =True)
y_axis_topvalues = y_axis[:10]
x_axis_topvalues=[]
print(y_axis_topvalues)
for val in y_axis_topvalues:
   print(list(dict1.keys())[list(dict1.values()).index(val)])
   x_axis_topvalues.append(list(dict1.keys())[list(dict1.values()).index(val)])
   del dict1[list(dict1.keys())[list(dict1.values()).index(val)]]
print(x_axis_topvalues)
plt.barh(x_axis_topvalues, y_axis_topvalues, color=['turquoise', 'lightseagreen', 'mediumturquoise'])
plt.yticks(fontsize=50)
plt.xticks(fontsize=50)
plt.title('Line chart')
plt.xlabel('Sources with maximum truths', fontsize = 50)
plt.ylabel('Number of truths given by each source', fontsize = 50)
plt.show()

data[(data['Source']=='person') & (data['Label']==2)].shape

#Visualizing the sources with maximum fake news
import matplotlib.pyplot as plt
data['Source'].value_counts()
ListSource = data['Source'].unique()
dict1={}
for i in ListSource:
  k = data[(data['Source']==i) & (data['Label']==0)].shape[0]
  dict1[i] = k 
print(dict1)  

from matplotlib.pyplot import figure

figure(figsize=(100, 20), dpi=100)

x_axis=[]
y_axis =[]
for i in dict1.keys():
  x_axis.append(i)
  y_axis.append(dict1[i])
y_axis.sort(reverse =True)
y_axis_topvalues = y_axis[:10]
x_axis_topvalues=[]
print(y_axis_topvalues)
for val in y_axis_topvalues:
   print(list(dict1.keys())[list(dict1.values()).index(val)])
   x_axis_topvalues.append(list(dict1.keys())[list(dict1.values()).index(val)])
   del dict1[list(dict1.keys())[list(dict1.values()).index(val)]]
print(x_axis_topvalues)
plt.barh(x_axis_topvalues, y_axis_topvalues, color=['turquoise', 'lightseagreen', 'mediumturquoise'])
plt.yticks(fontsize=50)
plt.title('Line chart')
plt.xlabel('Sources with maximum fake news', fontsize = 50)
plt.ylabel('Number of fake news given by each source', fontsize = 50)
plt.show()

# Removing stop words:
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = stopwords.words('english')
data['Claim']= data['Claim'].str.lower()
data['Claim'] = data['Claim'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
data['Claim']

import requests
from bs4 import BeautifulSoup

for index, row in data.iterrows():
    url = row['Fact-checked Article']
    try:
      reqs = requests.get(url,verify =False)   
      soup = BeautifulSoup(reqs.text, 'html.parser')
      title = soup.find('title')  
      if title:     
        data.loc[index, 'Headline'] = title.string
    except AttributeError:
          pass
    else:
       continue

#selected_rows = data[~data['Headline'].isnull()]
#print(selected_rows.shape)

#Plotting the wordclouds for different labels
# importing wordcloud
import matplotlib.pyplot as plt

from wordcloud import WordCloud

# make object of wordcloud
wc = WordCloud(background_color='white',min_font_size=10,width=500,height=500)

false_news_wc = wc.generate(data[data['Label'] == 0]['Claim'].str.cat(sep=" "))
plt.figure(figsize=(8,6))
plt.imshow(false_news_wc)
plt.show()

misleading_news_wc = wc.generate(data[data['Label'] == 1]['Claim'].str.cat(sep=" "))
plt.figure(figsize=(8,6))
plt.imshow(misleading_news_wc)
plt.show()

true_news_wc = wc.generate(data[data['Label'] == 2]['Claim'].str.cat(sep=" "))
plt.figure(figsize=(8,6))
plt.imshow(true_news_wc)
plt.show()

Unproven_news_wc = wc.generate(data[data['Label'] == 3]['Claim'].str.cat(sep=" "))
plt.figure(figsize=(8,6))
plt.imshow(Unproven_news_wc)
plt.show()

import seaborn as sns
from collections import Counter

# create list of True News words
true_news_words_list = data[data['Label']==2]['Claim'].str.cat(sep = " ").split()

# create DataFrame of that
true_news_words_df = pd.DataFrame(Counter(true_news_words_list).most_common(20))


sns.barplot(x=true_news_words_df[0],y=true_news_words_df[1])
plt.xticks(rotation='vertical')
plt.xlabel('Words')
plt.ylabel('Counts')
plt.title('Most frequently occuring words in truth')
plt.show()

import seaborn as sns

# create list of True News words
false_news_words_list = data[data['Label']==0]['Claim'].str.cat(sep = " ").split()

# create DataFrame of that
false_news_words_df = pd.DataFrame(Counter(false_news_words_list).most_common(20))

# Now Let's Plot barplot of this words
sns.barplot(x=false_news_words_df[0],y=false_news_words_df[1])
plt.xticks(rotation='vertical')
plt.xlabel('Words')
plt.ylabel('Counts')
plt.title('Most frequently occuring words in fake news')
plt.show()

#Performing tokenization
import nltk
nltk.download('punkt')
import nltk.data

data['Claim'] = data.apply(lambda x: nltk.word_tokenize(x['Claim']), axis=1)
data[['Claim']].head()

data['Combined Column'] = data['Source'].astype(str)+" "+ data['Country (mentioned)'].astype(str) +" "+data['Claim'].astype(str)
data2 = data['Combined Column']
data2.shape

from sklearn.model_selection import train_test_split
#dataX = data.drop(['Label'], axis =1)
X_train, X_test, Y_train, Y_test = train_test_split(data2, data['Label'], train_size = 0.8, random_state =0)
print(X_train.shape)
print(X_test.shape)

# create the transform
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words = 'english')

# transforming using TF-IDF
tfidf_train = vectorizer.fit_transform(X_train)
tfidf_train
tfidf_test = vectorizer.transform(X_test)
tfidf_test
tfidf_df = pd.DataFrame(tfidf_train.A, columns=vectorizer.get_feature_names())
tfidf_df

#K Nearest Neignbors Classifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, f1_score, ConfusionMatrixDisplay, recall_score
knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(tfidf_train, Y_train)
y_pred_test = knn.predict(tfidf_test)
print('Accuracy: %.3f' %accuracy_score(Y_test, y_pred_test))
print('Precision: %.3f' % precision_score(Y_test, y_pred_test, pos_label='positive', average='micro'))
print('Recall: %.3f' % recall_score(Y_test, y_pred_test, average='micro'))
print('F1 Score: %.3f' % f1_score(Y_test, y_pred_test, pos_label='positive', average='micro'))
ConfusionMatrixDisplay.from_estimator(knn, tfidf_test, Y_test)

# SVM Classifier
from sklearn import svm
from sklearn.metrics import accuracy_score, precision_score, f1_score, ConfusionMatrixDisplay, recall_score
svmclf = svm.SVC()
svmclf.fit(tfidf_train, Y_train)
y_pred_test = svmclf.predict(tfidf_test)
print('Accuracy: %.3f' %accuracy_score(Y_test, y_pred_test))
print('Precision: %.3f' % precision_score(Y_test, y_pred_test, pos_label='positive', average='micro'))
print('Recall: %.3f' % recall_score(Y_test, y_pred_test, average='micro'))
print('F1 Score: %.3f' % f1_score(Y_test, y_pred_test, pos_label='positive', average='micro'))
ConfusionMatrixDisplay.from_estimator(svmclf, tfidf_test, Y_test)

# Logistic Regression classifier
from sklearn.linear_model import LogisticRegression
LogisticRegressionClf = LogisticRegression(random_state=0)
LogisticRegressionClf.fit(tfidf_train, Y_train)
Y_pred_test = LogisticRegressionClf.predict(tfidf_test)
print('Accuracy: %.3f' %accuracy_score(Y_test,Y_pred_test ))
print('Precision: %.3f' % precision_score(Y_test, Y_pred_test, pos_label='positive', average='micro'))
print('Recall: %.3f' % recall_score(Y_test, y_pred_test, average='micro'))
print('F1 Score: %.3f' % f1_score(Y_test, Y_pred_test, pos_label='positive', average='micro'))
ConfusionMatrixDisplay.from_estimator(LogisticRegressionClf, tfidf_test, Y_test)

#AdaBoost Classifier
from sklearn.ensemble import AdaBoostClassifier
AdaBoostclf = AdaBoostClassifier(n_estimators =100, random_state =0)
AdaBoostclf.fit(tfidf_train, Y_train)
Y_pred_test = AdaBoostclf.predict(tfidf_test)
print('Accuracy: %.3f' %accuracy_score(Y_test,Y_pred_test ))
print('Precision: %.3f' % precision_score(Y_test, Y_pred_test, pos_label='positive', average='micro'))
print('Recall: %.3f' % recall_score(Y_test, y_pred_test, average='micro'))
print('F1 Score: %.3f' % f1_score(Y_test, Y_pred_test, pos_label='positive', average='micro'))
ConfusionMatrixDisplay.from_estimator(AdaBoostclf, tfidf_test, Y_test)

#Decision tree Classifier
from sklearn import tree
from sklearn.metrics import accuracy_score, precision_score, f1_score, ConfusionMatrixDisplay

DecisionTreeclf = tree.DecisionTreeClassifier(criterion ='entropy', max_depth=3)
DecisionTreeclf.fit(tfidf_train, Y_train)
Y_pred_test = DecisionTreeclf.predict(tfidf_test)
acc_score = accuracy_score(Y_test, Y_pred_test)
print('Accuracy: %.3f' %acc_score)
print('Precision: %.3f' % precision_score(Y_test, Y_pred_test, pos_label='positive', average='micro'))
print('Recall: %.3f' % recall_score(Y_test, y_pred_test, average='micro'))
print('F1 Score: %.3f' % f1_score(Y_test, Y_pred_test, pos_label='positive', average='micro'))
ConfusionMatrixDisplay.from_estimator(DecisionTreeclf, tfidf_test, Y_test)

#Importing the test data
testdata = pd.read_csv('/content/test.csv')

# Removing stop words:
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords


stop_words = stopwords.words('english')
testdata['Claim']= testdata['Claim'].str.lower()
testdata['Claim'] = testdata['Claim'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
testdata['Claim']

#Performing tokenization
import nltk
nltk.download('punkt')
import nltk.data

testdata['Claim'] = testdata.apply(lambda x: nltk.word_tokenize(x['Claim']), axis=1)
testdata[['Claim']].head()

# Combining Source, Country(mentioned) and claim into on column
testdata['Combined Column'] = testdata['Source'].astype(str)+" "+ testdata['Country (mentioned)'].astype(str) +" "+testdata['Claim'].astype(str)
testdata2 = testdata['Combined Column']
testdata2

# tfidf vectorizer
tfidf_testdata = vectorizer.transform(testdata2)
tfidf_testdata
tfidf_df = pd.DataFrame(tfidf_testdata.A, columns=vectorizer.get_feature_names())
tfidf_df.shape

import pandas as pd
y_pred_testdata = svmclf.predict(tfidf_testdata.todense())
y_pred_testdata
df = pd.DataFrame(y_pred_testdata)

df.columns = ['Category']
df.index.name = 'Id'
df.index += 1
df.to_csv('Group 7.csv')
df